receivers:
  otlp:
    protocols:
      grpc:
        endpoint: 0.0.0.0:14317
      http:
        endpoint: 0.0.0.0:14318

processors:
  ai_processor:
    models:
      error_classifier:
        path: "./models/error-classifier.wasm"
        memory_limit_mb: 100
        timeout_ms: 50
      importance_sampler:
        path: "./models/importance-sampler.wasm"
        memory_limit_mb: 80
        timeout_ms: 30
      entity_extractor:
        path: "./models/entity-extractor.wasm"
        memory_limit_mb: 150
        timeout_ms: 50
    processing:
      batch_size: 50
      concurrency: 4
      queue_size: 1000
      timeout_ms: 500
    features:
      error_classification: true
      smart_sampling: true
      entity_extraction: false
    sampling:
      error_events: 1.0
      slow_spans: 1.0
      normal_spans: 0.1
      threshold_ms: 500
    output:
      attribute_namespace: "ai."
      include_confidence_scores: true
      max_attribute_length: 256

exporters:
  otlp:
    endpoint: "localhost:4319"
    tls:
      insecure: true

service:
  telemetry:
    logs:
      level: debug
      development: true
      output_paths: ["stdout", "./processor.log"]
    metrics:
      address: 0.0.0.0:18888  # Custom Prometheus metrics port
  pipelines:
    traces:
      receivers: [otlp]
      processors: [ai_processor]
      exporters: [otlp]
    metrics:
      receivers: [otlp]
      processors: [ai_processor]
      exporters: [otlp]
    logs:
      receivers: [otlp]
      processors: [ai_processor]
      exporters: [otlp]